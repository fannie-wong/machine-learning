Backpropagation is using the chain rule to calculate the gradient(for Weights and Bias).

Here we first use a simple example to show how this algorithm works:

#input the initial value
x = -3; y = 6; z = -4
#feedforward
alpha = x + y  #alpha=3
f = alpha * z  #f=-12
#backpropagation:
#first: f = alpha * z
df/dz = alpha #the gradient of z is 3
df/dalpha = z #the gradient of alpha is -4
#then: alpha = x + y
#chain rule
df/dx = df/dalpha * dalpha/dx #dalpha/dx = 1 
df/dy = df/dalpha * dalpha/dy #dalpha/dy = 1

After calculating the gradients[df/dx,df/dy,df/dz], we can use them to update the original weights and bias.

Now, let us see the algorithm in neural network:

def backprop(self, x, y):
  nabla_b = [np.zeros(b.shape) for b in self.biases]
  nabla_w = [np.zeros(w.shape) for w in self.weights]
  # feedforward
  activation = x
  activations = [x] 
  zs = []
  for b, w in zip(self.biases, self.weights):
    z = np.dot(w, activation)+b
    zs.append(z)
    activation = sigmoid(z)
    activations.append(activation)
  # backward pass
  delta = self.cost_derivative(activations[‐1], y) * sigmoid_prime(zs[‐1])
  nabla_b[‐1] = delta
  nabla_w[‐1] = np.dot(delta, activations[‐2].transpose())
  for l in xrange(2, self.num_layers):
    z = zs[‐l]
    sp = sigmoid_prime(z)
    delta = np.dot(self.weights[‐l+1].transpose(), delta) * sp
    nabla_b[‐l] = delta
    nabla_w[‐l] = np.dot(delta, activations[‐l‐1].transpose())
return (nabla_b, nabla_w)
